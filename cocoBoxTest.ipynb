{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830cb741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "\n",
    "DATAPATH = '/mnt/NeuralNetworksDL/coco/'\n",
    "GRAYS = [498856, 6432, 84582, 457741, 11801, 427401, 821, 225717, 118895, 325387, 217886, 575029,\n",
    "         78250, 81003, 100896, 150354, 476888, 436984, 122051, 155083, 156878, 61048, 105872,\n",
    "         233263, 406404, 416869, 518025, 343009, 416372, 140627, 207339, 5294, 300200, 72098, 492325,\n",
    "         507794, 211867, 577207, 249711, 173610, 563447, 257178, 525513, 221691, 154053, 470442, 296884,\n",
    "         104124, 32405, 384907, 394322, 176397, 85407, 491058, 389984, 560349, 434837, 220770, 451074, 86,\n",
    "         406011, 406744, 134071, 269858, 410498, 53756, 46433, 363331, 280731, 140623, 204792, 80906, 33127,\n",
    "         132791, 228474, 571415, 361221, 208206, 342051, 349069, 377984, 155954, 451095, 532787, 573179,\n",
    "         155811, 27412, 124694, 336668, 577265, 185639, 103499, 532919, 510587, 145288, 559665, 176483, 342921,\n",
    "         64270, 123539, 205782, 205486, 57978, 353952, 312288, 397575, 439589, 431115, 126531, 287422,\n",
    "         555583, 173081, 380088, 401901, 579138, 260962, 166522, 426558, 421195, 361516, 390663, 15236, 30349,\n",
    "         107450, 385625, 29275, 443909, 250239, 134206, 226585, 518951, 131942, 1350, 93120, 509358, 561842, 131366,\n",
    "         386204, 268036, 217341, 6379, 549879, 564314, 111109, 434765, 35880, 381270, 330736, 384693, 39068, 18702,\n",
    "         316867, 186888, 264165, 389206, 15286, 445845, 58517, 470933, 33352, 210847, 458073, 377837, 293833,\n",
    "         25404, 95753, 270925, 463454, 443689, 213280, 563376, 77709, 243205, 313608, 210175, 566596, 60060,\n",
    "         259284, 263002, 576700, 484742, 66642, 341892, 400107, 394547, 12345, 75052, 39790, 369966, 134918,\n",
    "         505962, 39900, 179405, 34861, 220898, 450674, 223616, 454000, 540378, 3293, 492395, 249835, 429633,\n",
    "         520479, 579239, 537427, 449901, 358281, 384910, 494273, 140092, 321897, 347111, 571503, 503640, 64332,\n",
    "         421613, 113929, 10125, 8794, 107962, 496444, 480482, 264753, 87509, 40428, 517899]\n",
    "COCO_CLASSES = {'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4, 'airplane': 5, 'bus': 6,\n",
    "                'train': 7, 'truck': 8, 'boat': 9, 'traffic light': 10, 'fire hydrant': 11,\n",
    "                'stop sign': 13, 'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17, 'dog': 18,\n",
    "                'horse': 19, 'sheep': 20, 'cow': 21, 'elephant': 22, 'bear': 23, 'zebra': 24,\n",
    "                'giraffe': 25, 'backpack': 27, 'umbrella': 28, 'handbag': 31, 'tie': 32,\n",
    "                'suitcase': 33, 'frisbee': 34, 'skis': 35, 'snowboard': 36, 'sports ball': 37,\n",
    "                'kite': 38, 'baseball bat': 39, 'baseball glove': 40, 'skateboard': 41,\n",
    "                'surfboard': 42, 'tennis racket': 43, 'bottle': 44, 'wine glass': 46, 'cup': 47,\n",
    "                'fork': 48, 'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53,\n",
    "                'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57, 'hot dog': 58, 'pizza': 59,\n",
    "                'donut': 60, 'cake': 61, 'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65,\n",
    "                'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73, 'mouse': 74, 'remote': 75,\n",
    "                'keyboard': 76, 'cell phone': 77, 'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81,\n",
    "                'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86, 'scissors': 87,\n",
    "                'teddy bear': 88, 'hair drier': 89, 'toothbrush': 90}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530bd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image, ground_truths):\n",
    "\n",
    "        h, w = image.shape[1:]\n",
    "        new_h = int(self.output_size * h)\n",
    "        new_w = int(self.output_size * w)\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[:, top:top+new_h, left:left+new_w]\n",
    "\n",
    "        ground_truths = ground_truths - (left, top, 0, 0)\n",
    "\n",
    "        return image, ground_truths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8f9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/dataloader-collate-fn-throws-runtimeerror-stack-expects-each-tensor-to-be-equal-size-in-response-to-variable-number-of-bounding-boxes/117952\n",
    "def my_collate(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "class CocoDataSet(Dataset):\n",
    "    def __init__(self, annt_file, data_dir=DATAPATH, cats=None, size=10000, transform=None, fetch_type='intersection', crop_size=None, augmentation=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.crop_size = crop_size\n",
    "        self.augmentation = augmentation\n",
    "        coco = COCO(data_dir + 'annotations/' + annt_file)\n",
    "        self.cats = {k: idx for idx, k in enumerate([COCO_CLASSES[cat] for cat in cats])}\n",
    "        print('Categories:', self.cats)\n",
    "        catIds = coco.getCatIds(catNms=cats)\n",
    "        if fetch_type == 'intersection' or len(cats) <= 1:\n",
    "            imgIds = coco.getImgIds(catIds=catIds)\n",
    "        elif fetch_type == 'union':\n",
    "            imgIds = []\n",
    "            for cat in catIds:\n",
    "                temp = coco.getImgIds(catIds=cat)\n",
    "                imgIds += [x for x in temp if x not in imgIds]\n",
    "        else:\n",
    "            raise ValueError('Must fetch image intersection or union for multiple categories')\n",
    "        self.ids = [x for x in imgIds if x not in GRAYS]\n",
    "        annIds = coco.getAnnIds(imgIds=self.ids, catIds=catIds, iscrowd=None)\n",
    "        anns = coco.loadAnns(annIds)\n",
    "        self.labels = {x: [] for x in self.ids}\n",
    "        self.boxes = {x: [] for x in self.ids}\n",
    "        for d in anns:\n",
    "            self.boxes[d['image_id']].append(d['bbox'])\n",
    "            self.labels[d['image_id']].append(self.cats[d['category_id']])\n",
    "        if size > len(self.ids):\n",
    "            self.size = len(self.ids)\n",
    "        else:\n",
    "            for k in self.ids[size:]:\n",
    "                self.boxes.pop(k, None)\n",
    "            self.ids = self.ids[:size]\n",
    "            self.size = size\n",
    "        print('Dataset size:', self.size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        # __getitem__ actually reads the img content\n",
    "        image = torchvision.io.read_image(self.data_dir + '{:012}'.format(self.ids[index]) + '.jpg').to(torch.float32) / 255\n",
    "        target = {'boxes': np.array(self.boxes[self.ids[index]])}\n",
    "        if self.crop_size:\n",
    "            image, target['boxes'] = RandomCrop(self.crop_size)(image, target['boxes'])\n",
    "        for box_idx, box in enumerate(target['boxes']):\n",
    "            # relative box coordinates w.r.t image size so that they transform accordingly\n",
    "            target['boxes'][box_idx, :] = (box[0] / image.shape[2], box[1] / image.shape[1],\n",
    "                                           box[2] / image.shape[2], box[3] / image.shape[1])\n",
    "        # one-hot encoding\n",
    "        target['labels'] = np.eye(len(self.cats))[self.labels[self.ids[index]]]\n",
    "        if self.augmentation:\n",
    "            if np.random.rand() < 0.5:\n",
    "                image = torchvision.transforms.functional.hflip(image)\n",
    "                for box_idx in range(len(target['boxes'])):\n",
    "                    target['boxes'][box_idx, 0] = 1 - target['boxes'][box_idx, 0] - target['boxes'][box_idx, 2]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.augmentation:\n",
    "            rand = 2 * np.random.rand() - 1\n",
    "            image = torchvision.transforms.functional.adjust_brightness(image,  1 + rand * self.augmentation)\n",
    "            rand = 2 * np.random.rand() - 1\n",
    "            image = torchvision.transforms.functional.adjust_contrast(image,  1 + rand * self.augmentation)\n",
    "            rand = 2 * np.random.rand() - 1\n",
    "            image = torchvision.transforms.functional.adjust_saturation(image,  1 + rand * self.augmentation)\n",
    "            rand = np.random.rand() - 0.5\n",
    "            image = torchvision.transforms.functional.adjust_hue(image, rand * self.augmentation / 5)\n",
    "            pass\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def load_coco_dataset(batch_size=64, cats=None, size=10000, dim=64, fetch_type='intersection'):\n",
    "    # ImageNet normalization, Resizing to dim x dim\n",
    "    original_data = CocoDataSet('instances_train2017.json',\n",
    "                                cats=cats,\n",
    "                                data_dir=DATAPATH,\n",
    "                                size=size,\n",
    "                                fetch_type=fetch_type,\n",
    "                                transform=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.Resize((dim, dim))]))\n",
    "    \n",
    "    # Augmentation\n",
    "    augmented_data = CocoDataSet('instances_train2017.json',\n",
    "                                 cats=cats,\n",
    "                                 data_dir=DATAPATH,\n",
    "                                 size=size,\n",
    "                                 fetch_type=fetch_type,\n",
    "                                 crop_size=0.75,\n",
    "                                 augmentation=0.5,\n",
    "                                 transform=torchvision.transforms.Compose([\n",
    "                                     torchvision.transforms.Resize((dim, dim))]))\n",
    "    data = torch.utils.data.ConcatDataset((original_data, augmented_data))\n",
    "    print('Dataset with augmentation size:', len(data))\n",
    "    n_val = int(0.1 * len(data)) + 1\n",
    "    idx = torch.randperm(len(data))\n",
    "    train_dataset = Subset(data, idx[:-n_val])\n",
    "    valid_dataset = Subset(data, idx[-n_val:])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=my_collate)\n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa05ab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=14.50s)\n",
      "creating index...\n",
      "index created!\n",
      "Categories: {18: 0, 17: 1}\n",
      "Dataset size: 8279\n",
      "loading annotations into memory...\n",
      "Done (t=13.60s)\n",
      "creating index...\n",
      "index created!\n",
      "Categories: {18: 0, 17: 1}\n",
      "Dataset size: 8279\n",
      "Dataset with augmentation size: 16558\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "mean = [-1 * x / y for x, y in zip(mean, std)]\n",
    "std = [1 / x for x in std]\n",
    "unnormalize = torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "train_loader, valid_loader = load_coco_dataset(batch_size=32, cats=('dog', 'cat'), size=10000, dim=256,\n",
    "                                              fetch_type='union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "\n",
    "# https://stackoverflow.com/a/37437395\n",
    "for imgs, targets in train_loader:\n",
    "    if len(targets[0]['boxes']) < 2:\n",
    "        continue\n",
    "    fig, ax = plt.subplots()\n",
    "    img = imgs[0]\n",
    "    target = targets[0]\n",
    "    #print(img)\n",
    "    #print(unnormalize(img))\n",
    "    ax.imshow(img.permute(1, 2, 0), vmin=0, vmax=1)\n",
    "    for idx, box in enumerate(target['boxes']):\n",
    "        x = box[0] * img.shape[2]\n",
    "        y = box[1] * img.shape[1]\n",
    "        width = box[2] * img.shape[2]\n",
    "        height = box[3] * img.shape[1]\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor=\"r\", fill=False)\n",
    "        #print(x, y,  width, height)\n",
    "        ax.add_patch(rect)\n",
    "    #print(target['labels'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93153361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a489565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
