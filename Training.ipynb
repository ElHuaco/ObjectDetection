{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "DATAPATH = './models/'\n",
    "# GPU training\n",
    "DEVICE = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {DEVICE}.\")\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=15.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Categories: {19: 0, 16: 1, 17: 2, 18: 3, 1: 4, 3: 5}\n",
      "Dataset size: 10000\n",
      "loading annotations into memory...\n",
      "Done (t=14.19s)\n",
      "creating index...\n",
      "index created!\n",
      "Categories: {19: 0, 16: 1, 17: 2, 18: 3, 1: 4, 3: 5}\n",
      "Dataset size: 10000\n",
      "Dataset with augmentation size: 20000\n"
     ]
    }
   ],
   "source": [
    "from cocoBox import load_coco_dataset\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "IMG_SIZE = 300\n",
    "# Select image categories with which to train SSD\n",
    "categories = ('horse', 'bird', 'cat', 'dog', 'person', 'car')\n",
    "train_dataloader, valid_dataloader = load_coco_dataset(batch_size=BATCH_SIZE,\n",
    "                                                       size=10000,\n",
    "                                                       dim=IMG_SIZE,\n",
    "                                                       cats=categories,\n",
    "                                                       fetch_type='union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, valid_loader):\n",
    "    best_valid_loss = float('Inf')\n",
    "    print('{} starting training'.format(dt.datetime.now()))\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss = 0.0\n",
    "        for images, targets in train_loader:\n",
    "            images = torch.cat(list(map(lambda x: x.unsqueeze(0), images))).to(device=DEVICE)\n",
    "            coords, conf = model(images)\n",
    "            loss = loss_fn(coords, conf, targets['boxes'].to(device=DEVICE), targets['labels'].to(device=DEVICE))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.0\n",
    "            for images, targets in train_loader:\n",
    "                images = images.to(device=DEVICE)\n",
    "                coords, conf = model(images)\n",
    "                loss = SSDloss(coords, conf, targets['boxes'].to(device=DEVICE), targets['labels'].to(device=DEVICE))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                valid_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Verbose training\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Train {:.5f}, Valid {:.5f}'.format(dt.datetime.now(),\n",
    "                                                                      epoch,\n",
    "                                                                      train_loss,\n",
    "                                                                      valid_loss))\n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(model.state_dict(), DATAPATH + 'ssd.pt')\n",
    "            print(f'Saving {epoch}-th for {valid_loss = :2.5f}')\n",
    "            best_valid_loss = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered exception\n",
      "2023-01-17 20:47:19.727884 starting training\n",
      "Before base network: torch.Size([4, 3, 300, 300])\n",
      "After base network: torch.Size([4, 512, 38, 38])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m ssd \u001B[38;5;241m=\u001B[39m SSD(class_num\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(categories))\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mDEVICE)\n\u001B[1;32m     12\u001B[0m adam \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(ssd\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m \u001B[43mtraining_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m              \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madam\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m              \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSSDLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m              \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mssd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m              \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m              \u001B[49m\u001B[43mvalid_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalid_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 8\u001B[0m, in \u001B[0;36mtraining_loop\u001B[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, valid_loader)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, targets \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m      7\u001B[0m     images \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m), images)))\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mDEVICE)\n\u001B[0;32m----> 8\u001B[0m     coords, conf \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(coords, conf, targets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboxes\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mDEVICE), targets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mDEVICE))\n\u001B[1;32m     10\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/mnt/VisionCognitiveSystems/SSD.py:85\u001B[0m, in \u001B[0;36mSSD.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     83\u001B[0m x, scale4_offs, scale4_conf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale4(x) \n\u001B[1;32m     84\u001B[0m x, scale5_offs, scale5_conf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale5(x)\n\u001B[0;32m---> 85\u001B[0m _, scale6_offs, scale6_conf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale6\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     87\u001B[0m offs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((scale1_offs, scale2_offs, scale3_offs, scale4_offs, scale5_offs, scale6_offs), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     88\u001B[0m coords \u001B[38;5;241m=\u001B[39m offsets2coords(offs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredefined_boxes)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/mnt/VisionCognitiveSystems/SSD.py:29\u001B[0m, in \u001B[0;36mScaleMap.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     28\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(x)))\n\u001B[0;32m---> 29\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m     30\u001B[0m     _, _, h, w \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;66;03m# Prediction offset shape: Batch x (H * W * Priors) x 4\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import SSD\n",
    "importlib.reload(SSD)\n",
    "import VGG16\n",
    "importlib.reload(VGG16)\n",
    "from SSD import SSD\n",
    "import SSDLoss\n",
    "importlib.reload(SSDLoss)\n",
    "from SSDLoss import SSDLoss\n",
    "\n",
    "ssd = SSD(class_num=len(categories)).to(device=DEVICE)\n",
    "adam = torch.optim.Adam(ssd.parameters(), lr=1e-3)\n",
    "training_loop(n_epochs=1000,\n",
    "              optimizer=adam,\n",
    "              loss_fn=SSDLoss().to(device=DEVICE),\n",
    "              model=ssd,\n",
    "              train_loader=train_dataloader,\n",
    "              valid_loader=valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "# Load best models\n",
    "ssd.load_state_dict(torch.load(DATAPATH + 'ssd.pt', map_location=DEVICE))\n",
    "plt.figure(figsize=(12, 60))\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "mean = [-1 * x / y for x, y in zip(mean, std)]\n",
    "std = [1 / x for x in std]\n",
    "unnormalize = torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "for imgs, targets in valid_dataloader:\n",
    "    if len(targets[0]['boxes']) < 2:\n",
    "        continue\n",
    "    fig, ax = plt.subplots()\n",
    "    img = imgs[0]\n",
    "    target = targets[0]\n",
    "    ax.imshow(unnormalize(img).permute(1, 2, 0), vmin=0, vmax=1)\n",
    "    # TODO: add labelling of rectangle box\n",
    "    for idx, box in enumerate(target['boxes']):\n",
    "        x = box[0] * img.shape[2]\n",
    "        y = box[1] * img.shape[1]\n",
    "        width = box[2] * img.shape[2]\n",
    "        height = box[3] * img.shape[1]\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor=\"b\", fill=False)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x=x, y=y, s=categories[target['label'][idx]])\n",
    "    offs, conf = ssd.predict(imgs, targets)\n",
    "    # TODO: add labelling of rectangle box\n",
    "    for idx, box in enumerate(offs):\n",
    "        x = box[0] * img.shape[2]\n",
    "        y = box[1] * img.shape[1]\n",
    "        width = box[2] * img.shape[2]\n",
    "        height = box[3] * img.shape[1]\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor=\"r\", fill=False)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x=x, y=y, s=categories[np.argmax(conf[idx])])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}